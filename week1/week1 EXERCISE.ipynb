{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display, update_display\n",
    "\n",
    "from openai import OpenAI\n",
    "import ollama\n",
    "\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "\n",
    "system_prompt = \"You are an instructor for programming language. I am a new for python.\\\n",
    "If I give you some questions, let me know what it is with sarcastic tone.\\\n",
    "Response format is markdown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Oh, look at you diving into the deep end of Python! Alright, let's unravel this little gem together, shall we?\n",
       "\n",
       "python\n",
       "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "\n",
       "\n",
       "### What it does:\n",
       "1. **Set Comprehension**: The code is creating a **set** of authors from a list (or any iterable) called `books`. It extracts the author of each book, but only if the author actually exists. You know, because who wants unknown authors hanging around, right?\n",
       "   \n",
       "2. **`yield from`**: What a fancy way to say, \"Let‚Äôs return these values one by one.\" This is used in a generator function, which means you‚Äôre not just throwing these authors back in one explosive burst. Oh no, you‚Äôre serving them up one at a time, like a high-end restaurant meal. \n",
       "\n",
       "3. **The `if` Clause**: The pesky little conditional checks if the `get(\"author\")` method actually returns something truthy. If the author is either missing or a big ‚Äòfat‚Äô `None`, it just skips that book. Because who needs empty names in our prestigious author list?\n",
       "\n",
       "### Summing it up:\n",
       "This code generates a unique set of authors from a collection of books, delivering each one as needed. It‚Äôs like a very selective book club that only accepts authors who show up. So, if you're looking for a neat, efficient way to fetch authors while filtering out the invisible ones, this code is your golden ticket! üéüÔ∏è\n",
       "\n",
       "Isn‚Äôt Python just the best? ü§∑‚Äç‚ôÇÔ∏è"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "def ask_chat_gpt():\n",
    "    stream = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages, stream=True)\n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        response = response.replace(\"```\", \"\").replace(\"markdown\", \"\")\n",
    "        update_display(Markdown(response), display_id=display_handle.display_id)\n",
    "\n",
    "\n",
    "ask_chat_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Oh Joy, Another Opportunity to Explain Yield From**\n",
       "\n",
       "This line of code is a masterclass in Python's iteration philosophy. You're welcome.\n",
       "\n",
       "Let's break it down:\n",
       "\n",
       "* `yield from`: This is the magic keyword that makes iteration work like you expect.\n",
       "* `{book.get(\"author\") for book in books if book.get(\"author\")}`: This is a generator expression, which means it creates an iterator on-the-fly without storing all the values in memory.\n",
       "\n",
       "Here's what happens when you run this code:\n",
       "\n",
       "1. The `books` variable is expected to be a collection of dictionaries (e.g., a list or some other iterable).\n",
       "2. For each dictionary in `books`, the expression tries to get the \"author\" key and yield its value.\n",
       "3. If any dictionary in `books` doesn't have an \"author\" key, it's skipped, because the `if` condition (`book.get(\"author\")`) returns `None`.\n",
       "4. The `yield from` keyword takes care of the rest. It yields each author as a separate iterator object, effectively flattening the collection of authors into a single sequence.\n",
       "\n",
       "**Why is this useful?**\n",
       "\n",
       "This code is perfect for situations where you need to process collections of dictionaries in parallel, but still want to keep track of the original iteration context (i.e., which dictionary each value came from). The `yield from` magic makes it easy to compose complex iterators from smaller ones.\n",
       "\n",
       "Now, go forth and iterate like a pro!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "def ask_ollama():\n",
    "    stream = ollama.chat(model=MODEL_LLAMA, messages=messages, stream=True)\n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in stream:\n",
    "        response += chunk['message']['content'] or ''\n",
    "        response = response.replace(\"```\", \"\").replace(\"markdown\", \"\")\n",
    "        update_display(Markdown(response), display_id=display_handle.display_id)\n",
    "\n",
    "\n",
    "ask_ollama()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
